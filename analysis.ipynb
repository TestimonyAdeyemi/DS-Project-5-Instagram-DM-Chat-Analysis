{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: extract files from folder into one large folder. When I downloaded the data from instagram each conversation was stored in a different folder. But we need all our conversations in one folder. So we will extract our .json files(the conversations) from their individual folders and put it in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder where the files are currently located\n",
    "source_folder = \"inbox\"\n",
    "\n",
    "# Destination folder where all files will be moved\n",
    "destination_folder = \"data\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Iterate through all the subdirectories in the source folder\n",
    "for root, dirs, files in os.walk(source_folder):\n",
    "    for file in files:\n",
    "        # Check if the file is a JSON file\n",
    "        if file.endswith(\".json\"):\n",
    "            # Create the source file path\n",
    "            source_file_path = os.path.join(root, file)\n",
    "\n",
    "            # Create the destination file path by joining the destination folder path and the file name\n",
    "            destination_file_path = os.path.join(destination_folder, file)\n",
    "\n",
    "            # Handle filename conflicts by adding a number suffix\n",
    "            counter = 1\n",
    "            while os.path.exists(destination_file_path):\n",
    "                filename, ext = os.path.splitext(file)\n",
    "                new_filename = f\"{filename}_{counter}{ext}\"\n",
    "                destination_file_path = os.path.join(destination_folder, new_filename)\n",
    "                counter += 1\n",
    "\n",
    "            # Move the file from the source location to the destination location\n",
    "            shutil.move(source_file_path, destination_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Merge all files and dictonaries into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the folder path for \"data\"\n",
    "folder_path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "# List all the files in the \"data\" folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter JSON files from the list\n",
    "json_files = [file for file in file_list if file.endswith(\".json\")]\n",
    "\n",
    "# Dictionary to hold all the messages from different files\n",
    "all_messages_data = {}\n",
    "\n",
    "# Counter to keep track of the messages keys\n",
    "counter = 1\n",
    "\n",
    "# Iterate through each JSON file and create a new dictionary for each set of messages\n",
    "for json_file in json_files:\n",
    "    json_file_path = os.path.join(folder_path, json_file)\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    messages = data.get(\"messages\", [])\n",
    "\n",
    "    # Create a new key (messages1, messages2, messages3, etc.) for each set of messages\n",
    "    key = f\"messages{counter}\"\n",
    "    all_messages_data[key] = messages\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "# Save all the messages data into a new JSON file\n",
    "merged_file_path = os.path.join(os.getcwd(), \"merged_data.json\")\n",
    "with open(merged_file_path, \"w\") as merged_file:\n",
    "    json.dump(all_messages_data, merged_file, indent=4)\n",
    "\n",
    "print(\"Merged data saved to 'merged_data.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: reversed the data and saved them into rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the JSON file\n",
    "with open(\"merged_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a CSV file to store the processed data\n",
    "with open(\"test2.csv\", \"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row\n",
    "    header = [\"Dictionary Name\"]\n",
    "    max_messages = max(len(messages) for messages in data.values())\n",
    "    for i in range(1, max_messages + 1):\n",
    "        header.extend([f\"sender_name{i}\", f\"timestamp_ms{i}\", f\"content{i}\"])\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Process each dictionary in the merged data\n",
    "    for dictionary_name, messages in data.items():\n",
    "        messages = list(reversed(messages))  # Convert to a list and reverse the order of messages\n",
    "        row = [dictionary_name]\n",
    "        for message in messages:\n",
    "            sender_name = message.get(\"sender_name\", \"\")\n",
    "            timestamp_ms = message.get(\"timestamp_ms\", \"\")\n",
    "            content = json.dumps(message.get(\"content\", \"\"), ensure_ascii=False)\n",
    "            row.extend([sender_name, timestamp_ms, content])\n",
    "\n",
    "        # Pad the row with empty strings if the dictionary has fewer messages\n",
    "        padding = [''] * ((max_messages - len(messages)) * 3)\n",
    "        row.extend(padding)\n",
    "\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(\"Data processed and saved to 'test2.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: converting timestamps to date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_timestamp_to_datetime(timestamp):\n",
    "    try:\n",
    "        # Convert the timestamp to a datetime object\n",
    "        timestamp_ms = int(timestamp)  # Convert timestamp to integer if it's a string\n",
    "        datetime_obj = datetime.datetime.fromtimestamp(timestamp_ms / 1000)  # Convert from milliseconds to seconds\n",
    "        return datetime_obj.strftime('%Y-%m-%d %H:%M:%S')  # Format as desired\n",
    "    except ValueError:\n",
    "        return timestamp  # Return the original value if the conversion fails\n",
    "\n",
    "# Read the CSV file and load its data\n",
    "with open('test2.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "# Identify timestamp columns based on their labels and convert them\n",
    "header = data[0]\n",
    "timestamp_indices = [i for i, column in enumerate(header) if column.startswith('timestamp_ms')]\n",
    "for row in data[1:]:\n",
    "    for i in timestamp_indices:\n",
    "        row[i] = convert_timestamp_to_datetime(row[i])\n",
    "\n",
    "# Write the updated data back to the CSV file\n",
    "with open('output2.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(data)\n",
    "\n",
    "print(\"Timestamps converted to regular date and time format and saved to 'output2.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Extracting the relevant dated data, from August 1 to August 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open('output2.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Get the header row and find the index of the first timestamp column (timestamp_ms1)\n",
    "header = data[0]\n",
    "timestamp_index = header.index('timestamp_ms1')\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime.date(2023, 8, 1)\n",
    "end_date = datetime.date(2023, 8, 30)\n",
    "\n",
    "# Filter the data to keep only the rows where the first timestamp is within the specified date range\n",
    "filtered_data = [row for row in data[1:] if start_date <= datetime.datetime.strptime(row[timestamp_index], '%Y-%m-%d %H:%M:%S').date() <= end_date]\n",
    "\n",
    "# Filter the data to keep only the rows where the first timestamp is on or after July 1, 2023\n",
    "#filtered_data = [row for row in data[1:] if datetime.datetime.strptime(row[timestamp_index], '%Y-%m-%d %H:%M:%S').date() >= datetime.date(2023, 8, 23)]\n",
    "\n",
    "# Write the filtered data to a new CSV file\n",
    "with open('filtered_output.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(header)  # Write the header row\n",
    "    csv_writer.writerows(filtered_data)\n",
    "\n",
    "print(\"Messages with the first timestamp on or after August 1, 2023, are saved to 'filtered_output.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Filter out all conversations started by healthtracka, we want to deal with conversations started by leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open('filtered_output.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Get the header row and find the index of the sender_name1 column\n",
    "header = data[0]\n",
    "sender_name1_index = header.index('sender_name1')\n",
    "\n",
    "# Filter the data to keep only the rows where sender_name1 is not \"healthtracka\"\n",
    "filtered_data = [row for row in data[1:] if row[sender_name1_index] != \"Healthtracka\"]\n",
    "\n",
    "# Write the filtered data to a new CSV file\n",
    "with open('filtered_output2.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(header)  # Write the header row\n",
    "    csv_writer.writerows(filtered_data)\n",
    "\n",
    "print(\"Entries with sender_name1 as 'Healthtracka' are removed. Filtered data is saved to 'filtered_output2.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "Step 1: Finding out the number of text messages Healthtracka got during 1 Aug to 30 Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open('filtered_output2.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "     csv_reader = csv.reader(csv_file)\n",
    "     for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    " # Get the header row and find the index of the first timestamp column (timestamp_ms1)\n",
    "header = data[0]\n",
    "timestamp_index = header.index('timestamp_ms1')\n",
    "\n",
    "# Create a dictionary to store the count of messages for each date\n",
    "messages_count_by_date = {}\n",
    "\n",
    " # Count the messages for each date\n",
    "for row in data[1:]:\n",
    "    timestamp_str = row[timestamp_index]\n",
    "    date = datetime.datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S').date()\n",
    "    if date in messages_count_by_date:\n",
    "       messages_count_by_date[date] += 1\n",
    "    else:\n",
    "       messages_count_by_date[date] = 1\n",
    "\n",
    "# Plot the graph\n",
    "dates = list(messages_count_by_date.keys())\n",
    "counts = list(messages_count_by_date.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(dates, counts)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of new DMs')\n",
    "plt.title('Number of new DMs received each date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Analyzing the chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open('filtered_output2.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Get the header row and find the indices of columns\n",
    "header = data[0]\n",
    "content_indices = [i for i, col in enumerate(header) if col.startswith('content')]\n",
    "\n",
    "# Initialize a dictionary to store the number of filled columns for each conversation\n",
    "conversation_filled_counts = {}\n",
    "\n",
    "# Process each row in the data\n",
    "for row in data[1:]:\n",
    "    # Calculate the number of filled columns for the current conversation\n",
    "    num_filled_columns = sum(1 for content_index in content_indices if row[content_index].strip() != \"\")\n",
    "\n",
    "    # Find the sender_name1 for the current row\n",
    "    sender_name1 = row[1]  # Replace 1 with the correct index of sender_name1 in your CSV file\n",
    "\n",
    "    # Update the conversation_filled_counts dictionary\n",
    "    if sender_name1 != \"Healthtracka\":\n",
    "        if sender_name1 not in conversation_filled_counts:\n",
    "            conversation_filled_counts[sender_name1] = []\n",
    "\n",
    "        conversation_filled_counts[sender_name1].append(num_filled_columns)\n",
    "\n",
    "# Calculate the total filled columns for each sender_name1\n",
    "sender_names = []\n",
    "total_filled_columns = []\n",
    "for sender, filled_counts in conversation_filled_counts.items():\n",
    "    sender_names.append(sender)\n",
    "    total_filled_columns.append(sum(filled_counts))\n",
    "\n",
    "# Calculate the average, minimum, and maximum total filled columns\n",
    "sum_filled_columns = sum(total_filled_columns)\n",
    "average_filled_columns = sum(total_filled_columns) / len(total_filled_columns)\n",
    "min_filled_columns = min(total_filled_columns)\n",
    "max_filled_columns = max(total_filled_columns)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Number of chat messages received : {sum_filled_columns}\")\n",
    "print(f\"Average number of messages per Sender: {average_filled_columns:.2f}\")\n",
    "print(f\"Minimum number of messages per Sender: {min_filled_columns}\")\n",
    "print(f\"Maximum number of messages per Sender: {max_filled_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Listing out relevent sentences/messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def is_question(sentence):\n",
    "    # Check if any question word is present in the sentence\n",
    "    question_words = {\"health\", \"checkup\", \"package\", \"test\", \"price\", \"for\", \"it\", \"to\", \"i'll\", \"i\", \"much\", \"cost\",\n",
    "                      \"based\", \" in \", \"stay\", \"body\", \" full \", \"liver\",\n",
    "                      \"kidney\", \"heart\", \"my\", \" when \",\n",
    "                      \"check\", \"what\", \"will\", \"male\", \"fertility\", \"based\", \"pls\", \"please\", \"in\", \"need\", \"know\",\n",
    "                      \"can\", \"is\", \"or\", \"if\", \"when\", \"where\", \"who\", \"why\", \"how\", \"are\", \"do\", \"like\", \"book\",\n",
    "                      \"want\", \"full\", \"liver\", \"all\", \"std\", \"sti\", \"blood\", \"body\", \"check\", \"checkup\", \"is\"}\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    return any(word in question_words for word in words)\n",
    "\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open('filtered_output2.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Get the header row and find the indices of columns\n",
    "header = data[0]\n",
    "sender_name_indices = [i for i, col in enumerate(header) if col.startswith('sender_name')]\n",
    "content_indices = [i for i, col in enumerate(header) if col.startswith('content')]\n",
    "\n",
    "# Prepare the list to store questions with corresponding sender_name\n",
    "questions_with_senders = []\n",
    "\n",
    "# Initialize the total number of sentences counter\n",
    "total_sentences_count = 0\n",
    "\n",
    "# Process each row in the data\n",
    "for row in data[1:]:\n",
    "    for sender_index, content_index in zip(sender_name_indices, content_indices):\n",
    "        sender_name = row[sender_index]\n",
    "        content = row[content_index]\n",
    "\n",
    "        # Check if the sender_name is not \"healthtracka\"\n",
    "        if sender_name != \"Healthtracka\":\n",
    "            # Tokenize content into sentences\n",
    "            sentences = nltk.sent_tokenize(content)\n",
    "\n",
    "            # Increment the total sentences count\n",
    "            total_sentences_count += len(sentences)\n",
    "\n",
    "            # Print the content of each column with its index for the current conversation\n",
    "            # print(f\"Conversation with {sender_name} has {len(sentences)} sentences:\")\n",
    "            # for g, sentence in enumerate(sentences, 1):\n",
    "            #     print(f\"{g}. {sentence}\")\n",
    "\n",
    "            for sentence in sentences:\n",
    "                if is_question(sentence):\n",
    "                    questions_with_senders.append((sender_name, sentence))\n",
    "\n",
    "# Print the total number of sentences\n",
    "print(f\"Total number of sentences: {total_sentences_count}\")\n",
    "\n",
    "# Print the questions with corresponding sender_name for each content column\n",
    "print(\"Questions by Senders:\")\n",
    "for i, (sender, question) in enumerate(questions_with_senders, 1):\n",
    "    print(f\"{i}. Sender: {sender}, Question: {question}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Classifying sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def is_question(sentence):\n",
    "    # Check if any question word is present in the sentence\n",
    "    question_words = {\"health\", \"checkup\", \"package\", \"test\", \"price\", \"for\", \"it\", \"to\", \"i'll\", \"i\", \"much\", \"cost\", \"based\",\n",
    "                      \" in \", \"stay\", \"body\", \" full \", \"liver\",\n",
    "                      \"kidney\", \"heart\", \"my\", \"am\",\n",
    "                      \"check\", \"what\", \"will\", \"male\", \"fertility\", \"based\", \"pls\", \"please\", \"in\", \"need\", \"know\",\n",
    "                      \"can\", \"is\", \"or\", \"if\", \"when\", \"where\", \"who\", \"why\", \"how\", \"are\", \"do\", \"like\", \"book\",\n",
    "                      \"want\", \"full\", \"liver\", \"all\", \"std\", \"sti\", \"blood\", \"body\", \"check\", \"checkup\", \"is\"}\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    return any(word in question_words for word in words)\n",
    "\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = []\n",
    "with open('filtered_output2.csv', 'r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Get the header row and find the indices of columns\n",
    "header = data[0]\n",
    "sender_name_indices = [i for i, col in enumerate(header) if col.startswith('sender_name')]\n",
    "content_indices = [i for i, col in enumerate(header) if col.startswith('content')]\n",
    "\n",
    "# Prepare the list to store questions with corresponding sender_name\n",
    "questions_with_senders = []\n",
    "questions_with_senders1 = []\n",
    "questions_with_senders2 = []\n",
    "questions_with_senders3 = []\n",
    "questions_with_senders4 = []\n",
    "questions_with_senders5 = []\n",
    "questions_with_senders6 = []\n",
    "questions_with_senders7 = []\n",
    "\n",
    "# Process each row in the data\n",
    "for row in data[1:]:\n",
    "    for sender_index, content_index in zip(sender_name_indices, content_indices):\n",
    "        sender_name = row[sender_index]\n",
    "        content = row[content_index]\n",
    "\n",
    "        # Check if the sender_name is not \"healthtracka\"\n",
    "        if sender_name != \"Healthtracka\":\n",
    "            # Tokenize content into sentences\n",
    "            sentences = nltk.sent_tokenize(content)\n",
    "            for sentence in sentences:\n",
    "                if is_question(sentence) and any(word in sentence.lower() for word in\n",
    "                                                 [\"price\", \"much\", \"spend\", \"charge\", \"cost\", \"insurance\", \"free\", \"HMO\", \"pay\",\n",
    "                                                  \"payment\"]):\n",
    "                    questions_with_senders.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(word in sentence.lower() for word in [\"based\", \" in \", \"stay\", \"live\"]):\n",
    "                    questions_with_senders1.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(\n",
    "                        word in sentence.lower() for word in [\"body\", \" full \", \"liver\", \"kidney\", \"heart\", \"check\", \"general\", \"health\"]):\n",
    "                    questions_with_senders2.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(\n",
    "                        word in sentence.lower() for word in [\" std \", \" sti \", \"hpv\",  \"hiv\", \"herpes\", \"sexual health\", \"sexual\"]):\n",
    "                    questions_with_senders3.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(\n",
    "                        word in sentence.lower() for word in [\"couple\", \"wedding\", \"marriage\", \"fertility\"]):\n",
    "                    questions_with_senders4.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(word in sentence.lower() for word in\n",
    "                                                 [\"what\", \"type\", \"which\", \"service\",\"if\", \"do you\", \"asking\", \"offer\", \"know\", \"more\", \"i have\",\"does\", \"feeling\", \"pain\", \"my\"]):\n",
    "                    questions_with_senders5.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(\n",
    "                        word in sentence.lower() for word in [\"I would\", \"like\", \"I want\", \"to book\", \"want\", \"yes\", \"place\", \"can\", \"to test\", \"can i\", \"want\", \"would\", \"do i\", \"interested\"]):\n",
    "                    questions_with_senders6.append((sender_name, sentence))\n",
    "\n",
    "                if is_question(sentence) and any(word in sentence.lower() for word in [\"I have\",\n",
    "                                                                                       \" will \", \"are scheduled\", \"today\", \"booked\", \"have booked\", \"scheduled\",]) and \"cost\" not in sentence.lower() and \"like\" not in sentence.lower():\n",
    "                    questions_with_senders7.append((sender_name, sentence))\n",
    "\n",
    "# Save the output to a new CSV file\n",
    "with open('Statement_Classification.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write headers for the first category\n",
    "    writer.writerow([\"Statement Related to 'Price', 'Much', or 'Cost':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for i, (_, question) in enumerate(questions_with_senders, 1):\n",
    "        writer.writerow([i, question])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the second category\n",
    "    writer.writerow([\"Statement Related to 'in', 'at', or 'stay':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for g, (_, question1) in enumerate(questions_with_senders1, 1):\n",
    "        writer.writerow([g, question1])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the third category\n",
    "    writer.writerow([\"Statement Related to 'full', 'body', or 'check':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for j, (_, question2) in enumerate(questions_with_senders2, 1):\n",
    "        writer.writerow([j, question2])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the fourth category\n",
    "    writer.writerow([\"Statement Related to 'std', 'sti', or 'sexual':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for k, (_, question3) in enumerate(questions_with_senders3, 1):\n",
    "        writer.writerow([k, question3])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the fifth category\n",
    "    writer.writerow([\"Statement Related to 'couple', 'wedding', 'marriage':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for l, (_, question4) in enumerate(questions_with_senders4, 1):\n",
    "        writer.writerow([l, question4])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the sixth category\n",
    "    writer.writerow([\"Statement Related to 'what', 'type', 'which':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for m, (_, question5) in enumerate(questions_with_senders5, 1):\n",
    "        writer.writerow([m, question5])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the seventh category\n",
    "    writer.writerow([\"Statement Related to 'I would', 'I want', 'to book':\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for n, (_, question6) in enumerate(questions_with_senders6, 1):\n",
    "        writer.writerow([n, question6])\n",
    "\n",
    "    # Empty row for spacing between categories\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write headers for the eighth category\n",
    "    writer.writerow([\"Statement Related to 'will', 'I have' :\"])\n",
    "    writer.writerow([\"Index\", \"Question\"])\n",
    "    for o, (_, question7) in enumerate(questions_with_senders7, 1):\n",
    "        writer.writerow([o, question7])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
